<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>LECTURE 20: model selection</title>
    <meta charset="utf-8" />
    <meta name="author" content="   Fall 2022" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="FANR6750.css" type="text/css" />
    <link rel="stylesheet" href="FANR6750-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# LECTURE 20: model selection
]
.subtitle[
## FANR 6750 (Experimental design)
]
.author[
### <br/><br/><br/>Fall 2022
]

---




# outline 

&lt;br/&gt;

1) Motivation

&lt;br/&gt;

--

2) Approaches to model selection

&lt;br/&gt;

--

3) AIC



---
# motivation

#### As scientists, we usually have more than one hypothesis (or none at all!)  

&lt;br/&gt;

--

#### Consequently, we usually want to evaluate more than one model  

&lt;br/&gt;

--

#### **Model selection** is the process of choosing which model is most supported by our data

&lt;br/&gt;

--

#### Model selection is one of the most highly debated (and confusing) topics we will cover this semester

---
# model selection approaches

**Comparison of 2 (nested) models**  
- Likelihood-ratio test  


**Stepwise procedures**  
- Forward/backward/stepwise selection  


**Information-theortic approaches**  
- Akaike’s Information Criterion (AIC)  


**Cross-validation**  
- Leave-one-out  
- K-fold  


**Out-of-sample validation**  
- Compare predictions to new data

---
# questions

&lt;br/&gt;

#### How do we know which model is best?  

&lt;br/&gt;

--

#### Are any of them any good?  

&lt;br/&gt;


--

#### What is a good model?

&lt;br/&gt;

--

### The answers to these questions usually depend on *what* we want our models to do

---
# goals of modeling

### Exploration

- Identify potential relationships between variables

- *Generate* hypotheses

--

### Inference

- *Test* a priori hypotheses

--

### Prediction

- Predict response variable outside of observed data (usually to unobserved locations or future times)


--

#### The same data **cannot** be used for both exploration and inference 

---
# approaches to model selection

#### The most appropriate approach to model selection depends on what you want the model to do

--

#### The first step in any analysis is to determine what your goal is (exploration, inference, or prediction)

- Confusion about model selection often comes from not defining a clear goal

- Sometimes the goal will not be obvious 

--

#### Excellent paper:

&gt; Tredennick, A. T., Hooker, G., Ellner, S. P., &amp; Adler, P. B. (2021). A practical guide to selecting models for exploration, inference, and prediction in ecology. [Ecology, 102(6), e03336.](https://esajournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ecy.3336?casa_token=lJ3FPGPERrEAAAAA:0iXyzHKOWSJCr7lnwryWsJuXNTCITGhZa6bsRL5rSR8deXYWDmw99yhUPHomkVmf3iso_lZsahbXYEc)

---
class: middle, center, inverse

# inference

---
# inference

#### Inference is a central objective of science

- Using data to evaluate support for a hypothesis

--
&lt;/br&gt;

`$$\Large E(y_i) = \beta_0 + \beta_1  x^1_i + \beta_2  x^2_i$$`

&lt;/br&gt;

--

- Are coefficients ( `\(\beta_0\)`, `\(\beta_1\)`, `\(\beta_2\)` ) non-zero?

--

- Are coefficients positive or negative?

--

- Which covariate ( `\(x^1\)` or `\(x^2\)` ) has a larger effect on `\(y\)`?

--

#### The concepts we've learned this semester regarding null hypothesis testing and effect sizes are used for inference


---
# inference


#### For a well-designed study focused on inference, null hypothesis testing from a single model may be sufficient 

--

#### The main risk when conducting inference is Type I error

- For any single study/data set, significant results may be to due to sampling error

--

#### As a field, inference is strengthed by replication and validation

--

#### For a single study, risk of spurious results can be reduced by formulating strong hypotheses

- Covariates should be selected based on strong *a priori* reason to believe they influence response variable

- Where do *a priori* hypotheses come from? Theory (ideally) or...

---
class: middle, center, inverse

# exploration

---
# exploration

**Exploratory studies are a key part of the scientific process**

- Help identify *potential* relationships between variables

- Particularly common in observational studies with lots of covariates

- e.g., which weather covariates influence abundance of wildlife species?

--

**The main risk for exploratory analyses is detecting spurious relationships**

- Often desirable to "cast a wide net" for potential predictors (don't want to miss something important), but including more predictors increases risk of Type I error 

--

**Part of the [replication crisis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182327/) stems from treating exploratory analyses as if they are testing hypotheses**

- Evidence of a relationship should be a treated as a proposed hypothesis, which requires testing via **new** data sets (and ideally manipulative experiments)

---
class: middle, center, inverse

# prediction

---
# prediction

#### Historically, exploration and inference have been the primary focus in ecological research

- Out-sized emphasis on null hypothesis testing + very complex systems

- More prominent in other fields (meteorology, economics, political science)

--

#### Prediction is increasingly used to:

- Forecast ecological systems (e.g., population viability analysis)

- Test theory 

- Aid decision making (e.g., adaptive management)

---
# prediction

&lt;/br&gt;

`$$\Large E(y_i) = \beta_0 + \beta_1  x^1_i + \beta_2  x^2_i$$`

--

- *Inference* focuses on `\(\beta_0\)`, `\(\beta_1\)`, and `\(\beta_2\)`

--

- *Prediction* focuses on `\(E(y_i)\)`

&lt;/br&gt;
--

**Question**: Does a model that provides the best predictions also provide the best inference?

--

**Answer**: No! Because:

- Models that predict well might include many correlated but unimportant covariates

- Models that provide reliable inference may not include covariates relevant to prediction

---
# prediction

&lt;/br&gt;

### In general, we seek models that are as simple as possible (but not more so)

&lt;/br&gt;

--

### Why do we want simplicity?

---
# fit and over-fit

#### `\(\large R^2\)` is a measure of model fit  

--

#### Questions

- Does the addition of a new predictor variable always increase `\(R^2\)`?  

--

- Do we want the model with the highest `\(R^2\)`?  

--

- What is the harm in adding "extra" predictor variables?

&lt;br/&gt;

--

#### Overly-complicated models don’t predict well. They are too specific to a particular dataset.

---
# fit and over-fit

&lt;br/&gt;

&lt;img src="20_model_selection_files/figure-html/unnamed-chunk-1-1.png" width="648" style="display: block; margin: auto;" /&gt;

---
# prediction


#### Predictive ability is assessed by comparing *predicted* values to *observed* values

- But we can't use the same values for both prediction and validation

--

#### Ideally, we compare predictions to *out of sample* values

- Predict future values, gather data, compare values (e.g., [waterfowl management](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.5836))

&lt;img src="fig/mallard_prediction.jpg" width="280px" height="225px" style="display: block; margin: auto;" /&gt;



---
# prediction


#### Predictive ability is assessed by comparing *predicted* values to *observed* values

- But we can't use the same values for both prediction and validation


#### Ideally, we compare predictions to *out of sample* values

- Predict future values, gather data, compare values (e.g., [waterfowl management](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.5836))

- Divide data into "training" and "testing" (possibly repeated multiple times, i.e. *cross validation)

--

#### Predictive accuracy can also be approximated using *Information-theoretic* approaches



---
class: middle, inverse, center

# aic

---
# aic

#### Minus twice the (maximized) log-likelihood plus two times the number of parameters

`$$\Large AIC = -2L(\hat{\theta}, y) + 2K$$`

--

#### Or, when ordinary least squares (OLS) is used for estimation, AIC is based on the residual sums-of-squares (RSS):

`$$\Large AIC = n \log(RSS/n) + 2K$$`
--

#### The key is to recognize that

&gt; AIC = measure of fit + complexity penalty

&lt;br/&gt;

--

AIC is asymtotically equivalent to leave-one-out cross-validation

---
# aic in practice

1) Select a set of candidate models

--

2) Fit the models to the data (maximize the likelihood or minimize the RSS)  

--

3) Compute the AIC of each model  

--

4) Rank the models by AIC (lower AIC is better)  

--

5) Compute the difference in AIC scores between the best model, and every other model  

`$$\large \Delta_i = AIC_i - AIC_{min}$$`
--

6) Compute the Akaike weight of each model:

`$$\large w_i = \frac{e^{-0.5\Delta_i}}{\sum_i e^{-0.5 \Delta_i}}$$`

--

7) A model with `\(w = 0.6\)` is twice as likely to be the best model in the set as a model with `\(w = 0.3\)`

---
# presentation of results

&lt;br/&gt;

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt; Model &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; RSS &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; K &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; AIC &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; \(\Delta_{AIC}\) &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; w &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 300 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 113.8 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.0 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.98 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 320 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 122.3 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8.4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.02 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 330 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 125.4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 11.5 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 330 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 129.4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 15.5 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???

Residual sum of squares (RSS) replaced by log likelihood if using maximum likelihood estimation

---
# small sample size adjustment

&lt;br/&gt;
&lt;br/&gt;

#### The last term is the “bias adjustment term”

`$$\Large AIC_c = n \log(RSS/n) + 2K + \frac{2K(K + 1)}{n - K - 1}$$`

---
# notes about aic


#### AIC is not a test  

--

#### AIC is a relative measure. You can’t compare the AICs of models fit to different datasets  

--

#### AIC tells you about the *relative* predictive ability of the model set, not the *absolute* predictive ability  

- There will always be a model with the lowest AIC. But all of the models in the set could be terrible  

--

#### Because AIC is based on predictive ability, it will be more likely to select unimportant/spurious covariates than NHST

--

#### AIC is best suited for *exploratory* analyses (lot's of potential covariates w/o *a priori* hypotheses). If goal is inference, use NHST. If goal is prediction, use validation 










    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
