---
title: "LECTURE 19: model selection"
subtitle: "FANR 6750 (Experimental design)"
author: "<br/><br/><br/>Fall 2021"
output:
  xaringan::moon_reader:
    css: ["default", "FANR6750.css", "FANR6750-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', warning=FALSE, message=FALSE, 
                      fig.retina = 2, fig.width = 9, fig.height = 6)
library(WILD3810)
#library(emo)
library(kableExtra)
library(dplyr)
library(FANR6750)
# library(gganimate)
```

# outline 

<br/>

1) Motivation

<br/>

--

2) AIC

<br/>

--

3) Multi-model inference



---
# motivation

#### As scientists, we usually have more than one hypothesis  

<br/>

--

#### Consequently, we usually want to evaluate more than one model  

<br/>

--

#### **Model selection** is the process of choosing which model is most supported by our data

<br/>

--

#### In cases where more than one model is highly supported, we can make inferences from multiple models using model averaging

---
# questions

<br/>

#### How do we know which model is best?  

<br/>

--

#### Are any of them any good?  

<br/>

--

#### What is a good model?

---
# what is a good model?

### Explanation

> A good model should be a good approximation of reality. It should describe how things actually work  


In other words, a good model should describe the processes that give rise to the patterns we observe  

<br/>

--

### Prediction

> A good model should have good predictive abilities  

---
# explanation

#### As scientists, we want:

> “an explanation that is as simple as possible, but no simpler” - Einstein (paraphrased by Reader’s Digest!)  

<br/>
<br/>

--

#### Why do we want simplicity?

---
# fit and over-fit

#### $\large R^2$ is a measure of model fit  

--

#### Questions

- Does the addition of a new predictor variable always increase $R^2$?  

--

- Do we want the model with the highest $R^2$?  

--

- What is the harm in adding “extra” predictor variables?

<br/>

--

#### Overly-complicated models don’t predict well. They are too specific to a particular dataset.

---
# fit and over-fit

<br/>

```{r fig.height=5}
set.seed(123456)
x <- runif(10, 0, 10)

e.y <- 3.2 + 3 * x

y <- rnorm(10, e.y, 3)

df <- data.frame(x = x, y = y)

ggplot(df, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "loess", se = FALSE) +
  geom_abline(intercept = 3.2, slope = 3, color = "black")
```

---
# prediction


#### Predictive ability is assessed by comparing observed and expected values 

--

#### In non-scientific fields, we might not care about describing how things actually work. We might only care about prediction  

--

#### However, in scientific contexts we want models that are good at description and prediction  

--

#### Excellent paper:

> Tredennick, A. T., Hooker, G., Ellner, S. P., & Adler, P. B. (2021). A practical guide to selecting models for exploration, inference, and prediction in ecology. [Ecology, 102(6), e03336.](https://esajournals.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ecy.3336?casa_token=lJ3FPGPERrEAAAAA:0iXyzHKOWSJCr7lnwryWsJuXNTCITGhZa6bsRL5rSR8deXYWDmw99yhUPHomkVmf3iso_lZsahbXYEc)

---
# model selection approaches

**Comparison of 2 models**  
- Likelihood-ratio test  

--

**Stepwise procedures**  
- Forward/backward/stepwise selection  

--

**Information-theortic approaches**  
- Akaike’s Information Criterion (AIC)  

--

**Cross-validation**  
- Leave-one-out  
- K-fold  

--

**Out-of-sample validation**  
- Compare predictions to new data

---
class: middle, inverse, center

# aic

---
# aic

#### Minus twice the (maximized) log-likelihood plus two times the number of parameters

$$\Large AIC = -2L(\hat{\theta}, y) + 2K$$

--

#### Or, when ordinary least squares (OLS) is used for estimation, AIC is based on the residual sums-of-squares (RSS):

$$\Large AIC = n \log(RSS/n) + 2K$$
--

#### The key is to recognize that

> AIC = measure of fit + complexity penalty

<br/>

--

AIC is asymtotically equivalent to leave-one-out cross-validation

---
# aic in practice

1) Select a set of candidate models

--

2) Fit the models to the data (maximize the likelihood or minimize the RSS)  

--

3) Compute the AIC of each model  

--

4) Rank the models by AIC (lower AIC is better)  

--

5) Compute the difference in AIC scores between the best model, and every other model  

$$\large \Delta_i = AIC_i - AIC_{min}$$
--

6) Compute the so-called Akaike weight of each model:

$$\large w_i = \frac{e^{-0.5\Delta_i}}{\sum_i e^{-0.5 \Delta_i}}$$

--

7) A model with $w = 0.6$ is twice as likely to be the best model in the set as a model with $w = 0.3$

---
# presentation of results

<br/>

```{r}
aic.tab <- data.frame(Model = 1:4,
                      RSS = c(300, 320, 330, 330),
                      K = c(2, 3, 3, 5),
                      AIC = c(113.8, 122.3, 125.4, 129.4),
                      dAIC = c(0, 8.4, 11.5, 15.5),
                      w = c(0.98, 0.02, 0.0, 0.0))

aic.tab %>%
  kable(format = "html", booktab = TRUE, col.names = c("Model", "RSS", "K", "AIC", "\\(\\Delta_{AIC}\\)", "w"), escape = FALSE, align = "c")
```

???

Residual sum of squares (RSS) replaced by log likelihood if using maximum likelihood estimation

---
# small sample size adjustment

<br/>
<br/>

#### The last term is the “bias adjustment term”

$$\Large AIC_c = n \log(RSS/n) + 2K + \frac{2K(K + 1)}{n - K - 1}$$

---
# notes about aic


#### AIC is not a test  

--

#### AIC is a relative measure. You can’t compare the AICs of models fit to different datasets  

--

#### In other words, all models must be fit to the exact same data  

--

#### There will always be a model with the lowest AIC. But all of the models in the set could be terrible  

--

#### Always assess the fit of your best model!

---
# aic as an alternative to null hypothesis testing

#### The use of AIC avoids some of the limitations of null hypothesis testing, such as:  

--

- Null hypotheses are rarely true  

--

- A *p*-value tells us nothing about the alternative hypothesis  

--

- Specification of $\alpha$ is arbitrary  

--

- Statistical significance is a function of sample size  

--

- Statistical significance does not equal biological significance  

---
class: inverse, center, middle

# multi-model inference

---
# multi-model inference

<br/>

#### What if several models have similar weights?  

<br/>

--

#### Multi-model inference involves using all of the models in the set of models  

---
# multi-model inference

<br/>

#### The key is to do a weighted average of parameters or predictions  

$$\Large\bar{\hat{\theta}} = \sum_{i=1}^{R} \hat{\theta}_i w_i$$

<br/>

where $\hat{\theta}_i$ is a parameter or a predicted value from model $i$ in the set of $R$ models

---
# averaging parameters

<br/>

#### In the case of linear models, we might want to average the $\large \beta$ parameters  

--

#### But not all parameters are in every model. What do we do?

--

#### Two options:

1) Average estimates over models in which the parameter occurs (*bad idea*)  

--

2) Average estimates over all models, and set the estimate to 0 when the parameter is not in the model

---
# example

<br/>


```{r}
mmi <- data.frame(Model = c("fm7", "fm8", "Average"),
                  Intercept = c(30.7, 28.0, 29.9),
                  Elevation = c(0.0083, 0.012, 0.0089),
                  Elevation.sq = c("-", "-0.026", "???"),
                  Chaparral = c("-", "0.093", "???"),
                  Weight = c(0.80, 0.19, NA))

options(knitr.kable.NA = " ")

mmi %>%
  kable(format = "html", col.names = c("Model", "Intercept", "Elevation", "Elevation squared",
                                       "Chaparral", "weight"), align = "c", escape = FALSE)
```


---
# example

<br/>


```{r}
mmi2 <- data.frame(Model = c("fm7", "fm8", "Average"),
                  Intercept = c(30.7, 28.0, 29.9),
                  Elevation = c(0.0083, 0.012, 0.0089),
                  Elevation.sq = c(0, -0.026, -0.005),
                  Chaparral = c(0, 0.093, 0.018),
                  Weight = c(0.80, 0.19, NA))

options(knitr.kable.NA = " ")

mmi2 %>%
  kable(format = "html", col.names = c("Model", "Intercept", "Elevation", "Elevation squared",
                                       "Chaparral", "weight"), align = "c", escape = FALSE)
```

<br/>

--

Be aware that some people recommend that you never model average regression coefficients. However, it’s always fine to model average predictions  


---
# model-averaged predictions

#### Same concept, applied to predicted values instead of parameter estimates  

--

#### Example: predict jay abundance when in Oak habitat at 1000 m elevation, and 10% chaparral

<br/>

--

```{r}
pred <- data.frame(Model = c("fm7", "fm8", "Average"),
                  Pred = c(42.17, 45.95, 42.47),
                  Weight = c(0.80, 0.19, NA))

options(knitr.kable.NA = " ")

pred %>%
  kable(format = "html", col.names = c("Model", "Predicted abundance", "weight"), align = "c", escape = FALSE)
```

---
# model-averaged se and ci

<br/>

#### We now know how to use model averaging to compute $\large \hat{\theta}$

<br/>

--

#### But, how do we calculate the SE or CI?

---
# unconditional se


<br/>
<br/>

$$\Large SE(\bar{\hat{\theta}}) = \sum_{i=1}^{R} w_i \sqrt{var(\hat{\theta}_i + (\hat{\theta}_i - \bar{\hat{\theta}})^2}$$

---
# example

<br/>

```{r}

eg <- data.frame(Model = 1:3,
                 beta = c(1.2, 1.1, 1.4),
                 SE = c(0.3, 0.25, 0.26),
                 w = c(0.5, 0.3, 0.2))

eg %>%
    kable(format = "html", col.names = c("Model", "\\(\\hat{\\beta}\\)", "SE", "weight"), align = "c", escape = FALSE)
```

--

$$\large \bar{\hat{\beta}} = 1.2 \times 0.5 + 1.1 \times 0.3 + 1.4 \times 0.2 = 1.21$$

--

$$\begin{aligned}
  SE = &0.5 \sqrt{0.3^2 + (1.2-1.21)^2} + \\
  &0.3 \sqrt{0.25^2 + (1.1-1.21)^2} + \\
  &0.2 \sqrt{0.26^2 + (1.4-1.21)^2} \\
  &= 0.29
\end{aligned}$$

---
# relative variable importance

<br/>

#### Which of the predictor variables is most important?  

<br/>

--

#### Burnham and Anderson recommend using $\large w_+(\beta_j)$, the sum of the AIC weights over all models that include $\large \beta_j$

---
# example

<br/>

```{r}

eg <- data.frame(Model = 1:3,
                 beta1 = c(1.2, 1.1, 1.4),
                 beta1 = c(2.0, "-", "-"),
                 beta3 = c("-", 0.6, 0.8),
                 w = c(0.4, 0.3, 0.3))

eg %>%
    kable(format = "html", col.names = c("Model", "\\(\\hat{\\beta}_1\\)", 
                                         "\\(\\hat{\\beta}_2\\)", 
                                         "\\(\\hat{\\beta}_3\\)", "\\(w\\)"), align = "c", escape = FALSE)
```

--

#### Variable importance

$$\begin{aligned}
  w_+(\beta_1) = 0.4 + 0.3  + 0.3 & = 1.0\\
  w_+(\beta_2) = 0.4 & = 0.4\\
  w_+(\beta_3) = 0.3  + 0.3 & = 0.6
\end{aligned}$$

---
# summary

Information-theoretic approaches avoid many of the problems associated with null hypothesis testing

--

Focus is on:  

- Model comparision  

- Strength of evidence for each model  

- Estimates of effect sizes  

--

Requires much more thought than accept/reject approach, but just like any method, it is easy to abuse  

--

Additional references  

> Burnham, K.P. and D.R. Anderson. 2002. Model Selection and Multimodel Inference. Springer (well known book)


> [http://www.stats.ox.ac.uk/~ripley/ModelChoice.pdf](http://www.stats.ox.ac.uk/~ripley/ModelChoice.pdf) (good overview)









