---
title: "LECTURE 2: t-tests and null hypothesis testing"
subtitle: "FANR 6750 (Experimental design)"
author: "<br/><br/><br/>Fall 2021"
output:
  xaringan::moon_reader:
    css: ["default", "FANR6750.css", "FANR6750-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', warning=FALSE, message=FALSE, 
                      fig.retina = 2)
library(FANR6750)
source(here::here("R/zzz.R"))
# library(emo)
# library(gganimate)
```

class: inverse

# outline

<br/>
#### 1) One-sample t-test

<br/>  
--

#### 2) Null hypothesis testing

<br/> 
--

#### 3) Two-sample t-test

<br/> 
--

#### 4) Paired t-test

---
# one sample t-test

#### Context

- We want to know if a population mean ( $\mu$ ) differs from some value $\mu_0$    

--
<br/>  
- Formally, we want to test the null hypothesis: $\large H_0: \mu = \mu_0$  

--
- The alternative hypothesis is either:  

  + $\large H_a : \mu \neq \mu_0$ if we are using a two-tailed test; or  
  
  + $\large H_a : \mu > \mu_0$ if we are using a one-tailed test  

--
- Examples  
  + Is the average measurement error equal to zero?  
  
  + Is the average student taller than 5'?

???

$\mu_0$ is often zero but could be any value we are interested in treating as the null hypothesis

Under the alternative hypothesis for the one-tailed test, we could test that the population mean is either greater than **or** less than $\mu_0$

---
# procedure

1) Draw a random sample from a population (assumed to be normally distributed)  

--
<br/>
2) Compute the standard error of the mean:

$$\large SEM = \frac{s}{\sqrt{n}} = \frac{\sqrt{\frac{1}{n-1}\sum^n_{i=1}(y_i-\bar{y})^2}}{\sqrt{n}}$$
--
<br/>
3) Compute the t statistic:  

$$\large t = \frac{\bar{y} - \mu_0}{SEM}$$

--
<br/>
4) If $t$ is more extreme than the critical value(s), reject the null hypothesis  

--
<br/>
5) Equivalently, if the $p$-value is $< \alpha$, reject the null  

---
# basic idea

### Reject the null hypothesis if $t$ is larger than would be expected under the null hypothesis

--
### This will happen if:  

- The sample mean is far from 0

- And/or the SE is small

---
class: inverse, center, middle
# null hypothesis testing

---
# null hypothesis testing

<br/> 

### Question: 

> How do we know the likely values of $t$ under the null hypothesis?  

<br/>

--

### Answer: 

> Theory says that the test statistic will follow a $t$-distribution with $n - 1$ degrees of freedom, **if the null hypothesis is true**

---
## THE *t*-DISTRIBUTION

<br/>
```{r t, fig.width=8, fig.height=6}
curve(dt(x, 1), -5, 5, ylim=c(0, 0.5),
      xlab="t value", ylab="Probability density",
      cex.lab=1.5, lwd=1.5)
curve(dt(x, 5), -5, 5, add=TRUE, col="blue", lwd=1.5)
curve(dnorm(x, 0, 1), -5, 5, add=TRUE,  lty=3, lwd=1.5)
legend(-5, 0.5, c("Standard normal distribution", "t distribution (df=5)", "t distribution (df=1)"),
       lty=c(3, 1, 1), col=c("black", "blue", "black"), lwd=1.5)
```

---
### CRITICAL VALUES FOR $\large \alpha = 0.05$

<br/>
```{r cv, fig.width=8, fig.height=6}
op <- par(mai=c(0.8, 0.2, 0.2, 0.2))
curve(dt(x, df=18), -4, 4, xlab="t value", ylab="", yaxt="n",
      ylim=c(0,0.5),
      frame=FALSE , main="t distribution with df=18", cex.main=1.5)
xs1 <- seq(qt(.025, df=18), -4, by=-0.1)
ys1 <- dt(xs1, df=19)
xs2 <- seq(qt(.975, df=18), 4, by=0.1)
ys2 <- dt(xs2, df=19)
polygon(c(xs1, rev(xs1)), c(rep(0, length(xs1)), rev(ys1)), col=gray(0.7))
polygon(c(xs2, rev(xs2)), c(rep(0, length(xs2)), rev(ys2)), col=gray(0.7))
text(xs1[1], dt(0,18)/1.5, "critical value\nt=-2.10", pos=3)
text(xs2[1], dt(0,18)/1.5, "critical value\nt=2.10", pos=3)
arrows(xs1[1], dt(0,18)/1.5, xs1[1], ys1[1], length=0.1)
arrows(xs2[1], dt(0,18)/1.5, xs2[1], ys2[1], length=0.1)
#text(-3, dt(0,18), "t=-3", pos=3)
#text(3, dt(0,18), "t=3", pos=3)
#arrows(-3, dt(0,18), -3, dt(-3,18), length=0.1)
#arrows(3, dt(0,18), 3, dt(3,18), length=0.1)
par(op)

```


---
### $\Large p$-values

<br/>
```{r p, fig.width=8, fig.height=6}
op <- par(mai=c(0.8, 0.2, 0.2, 0.2))
curve(dt(x, df=18), -4, 4, xlab="t value", ylab="", yaxt="n",
      ylim=c(0,0.5),
      frame=FALSE , main="t distribution with df=18", cex.main=1.5)
xs1 <- seq(qt(.025, df=18), -4, by=-0.1)
ys1 <- dt(xs1, df=19)
xs2 <- seq(qt(.975, df=18), 4, by=0.1)
ys2 <- dt(xs2, df=19)
xs3 <- seq(qt(pt(-3,18), df=18), -4, by=-0.1)
ys3 <- dt(xs3, df=19)
xs4 <- seq(qt(pt(3,18), df=18), 4, by=0.1)
ys4 <- dt(xs4, df=19)
polygon(c(xs1, rev(xs1)), c(rep(0, length(xs1)), rev(ys1)), col=gray(0.7))
polygon(c(xs2, rev(xs2)), c(rep(0, length(xs2)), rev(ys2)), col=gray(0.7))
polygon(c(xs3, rev(xs3)), c(rep(0, length(xs3)), rev(ys3)), col="red")
polygon(c(xs4, rev(xs4)), c(rep(0, length(xs4)), rev(ys4)), col="red")
text(-3, dt(0,18), "t=-3\np=0.0038", pos=3)
text(3, dt(0,18), "t=3\np=0.0038", pos=3)
#text(-3, dt(0,18), "t=-3", pos=3)
#text(3, dt(0,18), "t=3", pos=3)
arrows(-3, dt(0,18), -3, dt(-3,18), length=0.1)
arrows(3, dt(0,18), 3, dt(3,18), length=0.1)
par(op)
```


---
### MORE ON $\Large p$-VALUES

<br/>
#### A $\large p$-value tells us how likely the null hypothesis is, given your observations  

--
<br/>
#### Our conclusion must be to either reject or "fail to reject" the null hypothesis  

--
<br/>
#### A $\large p$-value does not tell us how much evidence there is in favor of a particular difference in means

---
# one-tailed vs. two-tailed tests

<br/>
```{r one-tail, fig.width=8, fig.height=6}
op <- par(mai=c(0.8, 0.2, 0.2, 0.2))
curve(dt(x, df=18), -4, 4, xlab="t value", ylab="", yaxt="n",
      ylim=c(0,0.5),
      frame=FALSE , main="t distribution with df=18", cex.main=1.5)
#xs1 <- seq(qt(.025, df=18), -4, by=-0.1)
#ys1 <- dt(xs1, df=19)
xs2 <- seq(qt(.95, df=18), 4, by=0.1)
ys2 <- dt(xs2, df=19)
text(xs2[1], dt(0,18)/1.5, "critical value\nt=1.73", pos=3)
arrows(xs2[1], dt(0,18)/1.5, xs2[1], ys2[1], length=0.1)
#polygon(c(xs1, rev(xs1)), c(rep(0, length(xs1)), rev(ys1)), col=gray(0.7))
polygon(c(xs2, rev(xs2)), c(rep(0, length(xs2)), rev(ys2)), col=gray(0.7))
par(op)
```

---
# more on degrees of freedom

<br/>
> The degrees of freedom for a calculation on a set of numbers is the number of elements in the set (i.e., how many numbers there are) minus the number of different things you must know about the set
in order to complete the calculation  

<br/>

--
#### Example:

> Consider a set of n = 5 numbers. In the absence of any information about them, all ve are free to be any value. However, if you are also told that the sum of the set is 20, then only 4 of the numbers are free to be anything, but the fifth is constrained by your knowledge that the sum must be 20.
Hence, $df = n - 1 = 4$

---
class: inverse, center, middle

## TWO-SAMPLE t-TEST

---
### TWO-SAMPLE t-TEST

#### Concept  
- We want to determine if two population means differ  

--

- The null hypothesis is: $\large H_0 : \mu_1 = \mu_2$  

--

- The alternative hypothesis is either:  
  + $\large H_a : \mu_1 \neq \mu_2$ for a two-tailed test, or  
  + $\large H_a :  \mu_1 > \mu_2$ for a one-tailed test  

--

- Appropriate when:  
  + The two samples, one from each population, are independent  
  + Both populations are (approximately) normally distributed  
  + The population variances are unknown but are the same for both populations  

---
# procedure

1) Draw two random samples from two populations  

--
<br/>
2) Compute the standard error of the difference in means:  

$$\large SEDM = \sqrt{SEM_1^2 + SEM^2_2}$$

--
<br/>
3) Compute the t statistic:

$$\large t = \frac{\bar{y}_1 -\bar{y}_2}{SEDM}$$

--
<br/>
4) If t is more extreme than the critical values, reject the null hypothesis  

--
<br/>
5) Critical value is based on $\large t_{\alpha,df}=n_1+n_2-2$


---
# worked example

#### Question: 

- Is there a difference in the density of trees at low and high elevations?


--

#### Hypothesis: 

- Trees are more numerous at low elevations  

--

#### Field procedure:

- $\large n=10$ plots are sampled using randomly located belt transects 100m long $\times$ 10m wide at both high and low elevations

--

#### Data:

.pull-left[
`Low elevation: 16, 14, 18, 17, 29, 31, 14, 16, 22, 15`
]

.pull-right[
`High elevation: 2, 11, 6, 8, 0, 3, 19, 1, 6, 5`
]

---
# worked example

```{r box, fig.height=7, fig.width=7}
df <- data.frame(Trees = c(16, 14, 18, 17, 29, 31, 14, 16, 22, 15,
                           2, 11, 6, 8, 0, 3, 19, 1, 6, 5),
                 Elevation = factor(rep(c("Low", "High"), each = 10), levels = c("Low", "High")))
ggplot(df, aes(x = Elevation, y = Trees)) + geom_boxplot()
```

---
# worked example

.pull-left[
`Low elevation`  

`16, 14, 18, 17, 29,`  
`31, 14, 16, 22, 15`  
]

.pull-right[
`High elevation`   

`2, 11, 6, 8, 0,`  
`3, 19, 1, 6, 5`  
]  

--

- Mean of low group: $\large \bar{y}_L = 20.2$  

- Mean of high group: $\large \bar{y}_H = 6.1$  

--

- Standard deviation of low group: $\large s_L = 6.03$  

- Standard deviation of high group: $\large s_H = 5.63$  

--

- Standard error of difference in means $\large SEDM_1 = 2.61$  

--

- Test statistic: $\large t = (20.2 - 6.1)/2.61 = 5.4$  

--

- Critical value: $\large t_{0.95;df=10+10-2} = 1.73$, $\large p$-value: $< 0.001$  

--

#### Is this a one- or two-tailed test? Why?  

---
#  equal variance assumption

#### Are the variances of the two populations equal?

--

- This is an assumption of the two-sample t-test  

--

- Again, we use samples to make inferences about populations  

--

- Hypotheses:
  + $\large H_0 : \sigma^2_1 = \sigma^2_2$  
  + $\large H_a : \sigma^2_1 \neq \sigma^2_2$  

--

- Tested using a ratio of sample variances: $\large F = s^2_1/s^2_2$  

--

- This is always a two-tailed test  

#### Note: It makes life easier to place the larger variance in the numerator of this ratio

---
# f-distribution

#### A ratio of variances follows an $\large F$-distribution

.pull-left[
#### Properties:  

- $F > 0$  

- $F$-distribution is not symmetrical  

- Shape of distribution depends on an ordered pair of degrees of freedom, $df_1$
and $df_2$
]

.pull-right[
```{r f, fig.height=4, fig.width=4}
curve(df(x, 9, 9), 0, 5, xlab="F value", ylab="Probability density",
      main="F distribution with df=9,9")
```
]


---
# f-distribution

<br/>
```{r f2, fig.width=8, fig.height=6}
op <- par(mai=c(0.8, 0.2, 0.2, 0.2))
curve(df(x, 9, 9), 0, 5, xlab="F value", ylab="", yaxt="n",
      ylim=c(0,1),
      frame=FALSE , main="F distribution with df1=9 and df2=9", cex.main=1.5)
xs1 <- seq(qf(.025, 9, 9), 0, by=-0.01)
ys1 <- df(xs1, 9, 9)
xs2 <- seq(qf(.975, 9, 9), 5, by=0.01)
ys2 <- df(xs2, 9, 9)
polygon(c(xs1, rev(xs1)), c(rep(0, length(xs1)), rev(ys1)), col=gray(0.7))
polygon(c(xs2, rev(xs2)), c(rep(0, length(xs2)), rev(ys2)), col=gray(0.7))
text(xs1[1], 0.9, "critical value\nF=0.25", pos=3)
text(xs2[1], 0.9, "critical value\nF=4.1", pos=3)
arrows(xs1[1], 0.9, xs1[1], ys1[1], length=0.1)
arrows(xs2[1], 0.9, xs2[1], ys2[1], length=0.1)
par(op)
```

---
# continuing with tree example

#### Test statistic: $\large F = 6.032/5.632 = 1.07$

--

#### Degrees of freedom: $\large df = 9, 9$  

--

#### Critical value: $\large F_{0.975;df=9;9} = 4.03$  

--

#### Decision: Observed $F$ is lower than critical value. Fail to reject the null. No strong evidence that variances are different.

---
class: inverse, middle, center

## PAIRED *t*-TEST

---
### PAIRED *t*-TEST

#### Context

- Used when two measurements are taken on each experimental unit  

--

- Problem can be analyzed by taking differences of each pair and then conducting a one-sample *t*-test  

--

- Examples:  
  + Are right feet usually longer than left feet?  
  + Is small mammal density higher before or after the use of prescribed fire?  
  + Do two methods of measuring tree height yield similar results?  

---
# motivation

<br/>
<br/>

> Matching is done in a variety of ways, but the object is always to remove extraneous variability from the experiment

---
# worked example

> Plots were arranged in pairs at 12 different locations. One plot in each pair was randomly selected for treatment with the microbial pesticide *Bacillus thuringiensis* (Bt). The other plot was untreated. Surveys of nontarget caterpillars were performed by counting caterpillars on samples of 10,000 leaves on each plot. Data below are caterpillar counts on each plot, paired by location.  

```{r tab}
library(kableExtra)
tab <- data.frame(Location = 1:12,
                  Untreated = c(23, 18, 29, 22, 33, 20, 17, 25, 27, 30, 25, 27),
                  Treated = c(19, 18, 24, 23, 31, 22, 16, 23, 24, 26, 24, 28),
                  Difference = c(4, 0, 5, -1, 2, -2, 1, 2, 3, 4, 1, -1))

tab %>%
  kable("html", align = 'c') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width = F, font_size = 10) %>%
  scroll_box(height = "300px")
```

---
# worked example

```{r box2, fig.height=7, fig.width=7}
untreated <- c(23, 18, 29, 22, 33, 20, 17, 25, 27, 30, 25, 27)
treated <- c(19, 18, 24, 23, 31, 22, 16, 23, 24, 26, 24, 28)
#diffs <- c(4,0,5,-1,2,-2,1,2,3,-6,1,-1)
diffs <- untreated - treated
#boxplot(untreated, treated)
#hist(diffs)
#plot(untreated, treated, asp=1)
#abline(1, 1)
boxplot(diffs, #names="Difference between untreated and treated",
        main="Boxplot of differences", col="tan", cex.lab=1.5,
        ylab="Caterpillers (Untreated - Treated)")
#abline(h=0, lty=2, col=4)
```

---
# worked example

#### Hypotheses ( $\large \mu_d$ is the mean difference )

- $\large H_0 :\mu_d = 0$

- $\large H_a :\mu_d > 0$

--

#### Calculations

- Mean differences: $\large\bar{y}_d = 1.5$  

- Standard deviation of differences: $\large s_d = 2.24$  

- Standard error of mean differences: $\large SEDM = 0.65$  

- Test statistic: $\large t = 1.5/0.65 = 2.32$, Critical value: $\large t_{0.95;11} = 1.80$

--

### Decision?


---
# looking ahead

<br/>

#### **Next time:** Completely randomized ANOVA

<br/>

#### **Reading:** Chapter 8 from Dowdy et al.

